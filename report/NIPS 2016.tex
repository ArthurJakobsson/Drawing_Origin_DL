\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Quick Draw! Predicting the Country}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Arthur K. Jakobsson\thanks{\href{https://arthurjakobsson.github.io/}{https://arthurjakobsson.github.io/}} \\
  Carnegie Mellon University\\
  Pittsburgh, PA 15213 \\
  \texttt{ajakobss@cmu.edu} \\
  \And
  Prathik Guduri \\
  Carnegie Mellon University \\
  Pittsburgh, PA 15213 \\
  \texttt{pguduri@andrew.cmu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Drawing is a skill that humans have used since the beginning of humanity
  to convey ideas and stories. However, the similarity between a 3rd grader's
  cartoon of a deer, a caveman's drawing of one and a photo of a deer vary
  drastically and yet we understand all three to represent the same subject. In this
  paper we seek to create a network to predict the category of drawings
  and we propose a network to abstract images into drawings (including
  the series of strokes required to derive the images).
\end{abstract}

\section{Introduction}

We have collected data from a publically available dataset produced by Google's
creative lab.
\begin{center}
  \url{https://github.com/googlecreativelab/quickdraw-dataset}
\end{center}

This dataset was created through an online game, ``Quick Draw!'' in which participants
are given a category to draw and 20 seconds to represent the topic. The game
then uses a neural network to try and predict the subject of the drawing. Each
drawing made, it's associated topic, the drawer's place of origin, the strokes
used to create the drawing and some other information are stored in the dataset
and are used to improve Google's performance for the game.

We intend to take this dataset preliminarily to recreate and improve upon
the predictive model created by Google in order to find the categories. However,
our end goal described in the Future Work section describes how we intend
to use the data combined with an image dataset of references for the categories
to predict how humans would draw an unseen image.


\section{Background Literature}

There are major variations between categories and other atributes within our
dataset. Fernandez-Fernandez et. al. [1] describes a brief statistical
analysis of the Quick Draw! dataset. They primarily focus on the level of complexity
of the drawings as described by the number of strokes used to represent the
drawing. They suggest that as a result of having much fewer strokes, some
categories may lack the complexity to be discriminated from others.

The ability for the model to create a drawing has been extensively looking into. Ha et. al.
[2] provides an example to create a robust method of sketch generation in a vector
format. Similarly Xie et. al. [3] from 2012 provides a baseline method based
on painting to try to estimate the brush strokes required to paint
an image using policy iteration and RL. Our final product
is inspired by these methods to take input images to draw the image
however we intend to use a new model architecture to better discriminate the images
and to train our generative model. This is futher described in the future
works section.

Similar models to ours have been created online to guess the identity of a drawing
based on the Quick Draw dataset. For example, [4] provides a baseline
convolutional framework that demonstrates similar, but slightly lower accuracy
(but on a larger dataset) to ours.

\section{Methods/Model}


\section{Preliminary Results}


\section{Evaluation of Preliminary Work}


\section{Future Work}

Our intended final product flips the current model on its head. We intend
to connect the current Quick Draw! dataset to a singular or multiple image
datasets that represent the categories drawn. From this we intend to use a
Generative Adversarial Network to create drawings from the images provided by
category. We hypothesize that our model will be able to learn features such
as the keypoints that humans jump to such as whiskers on a cat, eyes on a human
and other important features that are put into a recognizable sketch. The GAN will of course
have a discriminator which will take in the source image and the drawing produced (or the
true drawings from Quick Draw!) and try to detect if it was a drawing we created
or a source image. Through this we will train out generative side. We also
intend on adding other information that may help the model train such as an
edge detection map of the images.

We hope to produce a product that is not just a drawing algorithm but instead
learns the features that humans consider recognizable in a drawing. The incentive
behind this is that a drawing of a cat is composed of two eyes and 6 whiskers
which arguably look nothing like a cat, but is universally recognized as a drawing
of one, similarly a pig has two circles as a nose. Currently existing models,
try to create accurate representations of drawings from the image but do not
try to describe how we as humans "sketch" and break an image into its basic
topics.


\section{Teammates and Work Division}

\textit{Past work division:}

We worked together to create the network, coding live with each other and therefore put
in the same time and work on the project. The writing of this paper is divided equally
between us.
We both performed background research and brainstorming for this and then we
discussed and developed the idea and motivation behind the project together.
The background research and project setup was led by Arthur and the development
of the training loop and code structure was led by Prathik.

\textit{Future work division:}

We intend to continue working side-by-side heavily to develop our final model.
We will likely divide the work into topics of GAN development, evalution, ablation study
and writing. Our current proposal for division is Arthur leads the generative side of the GAn and the ablation
while Prathik leads the discriminatory network and the evaluation.

\section{Citations}

We developed our code using the Quick Draw! datset and the helper code provided
within it such as the code used to create the 28x28 images which we downloaded
directly.

We took inspiration from Arthur's team project in the robotics institute with
Rishi Veerapaneni <rveerapa@andrew.cmu.edu> to help us develop a pytorch
framework.

We also used

\begin{center}
  \url{https://machinelearningmastery.com/building-a-convolutional-neural-network-in-pytorch/}
\end{center}

as an example of a working CNN. We developed the structure of the code
heavily to meet our needs but we took the general torch CNN framework as inspiration
from this article.

\section*{References}

\small

[1] Fernandez-Fernandez, R.\ \& Victores, J.G.\ \& Estevez, D.\ \& Balaguer, C.\ (2019) Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset.
\ {\it Provides statistical background
to Quick Draw! Dataset} Universidad Carlos III de Madrid (UC3M)

[2] Ha D.\ \& Eck, D.\ (2017) {\it A Neural Representation of Sketch Drawings.}
Google.

[3] Xie, N.\ \& Hachiya, H.\ \& Sugiyama, M\ (2012) Artist Agent: A Reinforcement Learning
Approach to Automatic Stroke Generation in Oriental Ink Painting. {\it Proceedings of the 29th International Conference
on Machine Learning} Department of Computer Science, Tokyo Institute of Technology

[4] kolszewska.\ Github. https://github.com/kolszewska/QuickDrawClassifier

\end{document}



% \subsection{Citations within the text}


% The style files for NIPS and other conference information are
% available on the World Wide Web at
% \begin{center}
%   \url{http://www.nips.cc/}
% \end{center}
% The file \verb+nips_2016.pdf+ contains these instructions and
% illustrates the various formatting requirements your NIPS paper must
% satisfy.

% The only supported style file for NIPS 2016 is \verb+nips_2016.sty+,
% rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{}
%   2.09, Microsoft Word, and RTF are no longer supported!}

% The new \LaTeX{} style file contains two optional arguments:
% \verb+final+, which creates a camera-ready copy, and \verb+nonatbib+,
% which will not load the \verb+natbib+ package for you in case of
% package clash.

% At submission time, please omit the \verb+final+ option. This will
% anonymize your submission and add line numbers to aid review.  Please
% do \emph{not} refer to these line numbers in your paper as they will
% be removed during generation of camera-ready copies.

% The file \verb+nips_2016.tex+ may be used as a ``shell'' for writing
% your paper. All you have to do is replace the author, title, abstract,
% and text of the paper with your own.

% The formatting instructions contained in these style files are
% summarized in Sections \ref{gen_inst}, \ref{headings}, and
% \ref{others} below.

% The text must be confined within a rectangle 5.5~inches (33~picas)
% wide and 9~inches (54~picas) long. The left margin is 1.5~inch
% (9~picas).  Use 10~point type with a vertical spacing (leading) of
% 11~points.  Times New Roman is the preferred typeface throughout, and
% will be selected for you by default.  Paragraphs are separated by
% \nicefrac{1}{2}~line space (5.5 points), with no indentation.

% The paper title should be 17~point, initial caps/lower case, bold,
% centered between two horizontal rules. The top rule should be 4~points
% thick and the bottom rule should be 1~point thick. Allow
% \nicefrac{1}{4}~inch space above and below the title to rules. All
% pages should start at 1~inch (6~picas) from the top of the page.

% For the final version, authors' names are set in boldface, and each
% name is centered above the corresponding address. The lead author's
% name is to be listed first (left-most), and the co-authors' names (if
% different address) are set to follow. If there is only one co-author,
% list both author and co-author side by side.

% Please pay special attention to the instructions in Section \ref{others}
% regarding figures, tables, acknowledgments, and references.

% The \verb+natbib+ package will be loaded for you by default.
% Citations may be author/year or numeric, as long as you maintain
% internal consistency.  As to the format of the references themselves,
% any style is acceptable as long as it is used consistently.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% If you wish to load the \verb+natbib+ package with options, you may
% add the following before loading the \verb+nips_2016+ package:
% \begin{verbatim}
%    \PassOptionsToPackage{options}{natbib}
% \end{verbatim}

% If \verb+natbib+ clashes with another package you load, you can add
% the optional argument \verb+nonatbib+ when loading the style file:
% \begin{verbatim}
%    \usepackage[nonatbib]{nips_2016}
% \end{verbatim}

% As submission is double blind, refer to your own published work in the
% third person. That is, use ``In the previous work of Jones et
% al.\ [4],'' not ``In our previous work [4].'' If you cite your other
% papers that are not widely available (e.g., a journal paper under
% review), use anonymous author names in the citation, e.g., an author
% of the form ``A.\ Anonymous.''

% \subsection{Footnotes}

% Footnotes should be used sparingly.  If you do require a footnote,
% indicate footnotes with a number\footnote{Sample of the first
%   footnote.} in the text. Place the footnotes at the bottom of the
% page on which they appear.  Precede the footnote with a horizontal
% rule of 2~inches (12~picas).

% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction. The figure number and caption
% always appear after the figure. Place one line space before the figure
% caption and one line space after the figure. The figure caption should
% be lower case (except for first word and proper nouns); figures are
% numbered consecutively.

% You may use color figures.  However, it is best for the figure
% captions and the paper body to be legible if the paper is printed in
% either black/white or in color.
% \begin{figure}[h]
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible.  The table
% number and title always appear before the table.  See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.

% Note that publication-quality tables \emph{do not contain vertical
%   rules.} We strongly suggest the use of the \verb+booktabs+ package,
% which allows for typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.

% \begin{table}[t]
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule{1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \section{Final instructions}

% Do not change any aspects of the formatting parameters in the style
% files.  In particular, do not modify the width or length of the
% rectangle the text should fit into, and do not change font sizes
% (except perhaps in the \textbf{References} section; see below). Please
% note that pages should be numbered.

% \section{Preparing PDF files}

% Please prepare submission files with paper size ``US Letter,'' and
% not, for example, ``A4.''

% Fonts were the main cause of problems in the past years. Your PDF file
% must only contain Type 1 or Embedded TrueType fonts. Here are a few
% instructions to achieve this.

% \begin{itemize}

% \item You should directly generate PDF files using \verb+pdflatex+.

% \item You can check which fonts a PDF files uses.  In Acrobat Reader,
%   select the menu Files$>$Document Properties$>$Fonts and select Show
%   All Fonts. You can also use the program \verb+pdffonts+ which comes
%   with \verb+xpdf+ and is available out-of-the-box on most Linux
%   machines.

% \item The IEEE has recommendations for generating PDF files whose
%   fonts are also acceptable for NIPS. Please see
%   \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

% \item \verb+xfig+ "patterned" shapes are implemented with bitmap
%   fonts.  Use "solid" shapes instead.

% \item The \verb+\bbold+ package almost always uses bitmap fonts.  You
%   should use the equivalent AMS Fonts:
% \begin{verbatim}
%    \usepackage{amsfonts}
% \end{verbatim}
% followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or
% \verb+\mathbb{C}+ for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You
% can also use the following workaround for reals, natural and complex:
% \begin{verbatim}
%    \newcommand{\RR}{I\!\!R} %real numbers
%    \newcommand{\Nat}{I\!\!N} %natural numbers
%    \newcommand{\CC}{I\!\!\!\!C} %complex numbers
% \end{verbatim}
% Note that \verb+amsfonts+ is automatically loaded by the
% \verb+amssymb+ package.

% \end{itemize}

% If your file contains type 3 fonts or non embedded TrueType fonts, we
% will ask you to fix it.

% \subsection{Margins in \LaTeX{}}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+ from the \verb+graphicx+ package. Always
% specify the figure width as a multiple of the line width as in the
% example below:
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% See Section 4.4 in the graphics bundle documentation
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

% A number of width problems arise when \LaTeX{} cannot properly
% hyphenate a line. Please give LaTeX hyphenation hints using the
% \verb+\-+ command when necessary.

% \subsubsection*{Acknowledgments}

% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments go at the end of the paper. Do not include
% acknowledgments in the anonymized submission, only in the final paper.
